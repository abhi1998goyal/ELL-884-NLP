{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8084644,"sourceType":"datasetVersion","datasetId":4772276}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## How to get high score using MMBT and CLIP in Hateful Memes Competition","metadata":{}},{"cell_type":"markdown","source":"#### Use CLIP as a feature encoder for Multimodal Bitransformer and make MMBT really work with Huggingface Transformers to get surprisingly high accuracy","metadata":{}},{"cell_type":"markdown","source":"The additional stage of <a href=https://www.drivendata.org/competitions/64/hateful-memes/>Hateful Memes Competition</a> from Facebook ended a few months ago. My team was lucky enough to take part in this competition and even get pretty good results (we took tenth place). How we did it and what methods we used - I'll tell you in this article. ","metadata":{}},{"cell_type":"markdown","source":"## Problem description","metadata":{}},{"cell_type":"markdown","source":"At first glance, the problem that had to be solved in the competition is quite simple - to determine whether a meme is hateful or not using text and image data from it. In reality, the problem is complicated by the many ambiguities inherent in our speech, as well as by the presence of sarcasm and irony, with the definition of which neural networks have problems. You can read more about the competition and the tasks it posed in the corresponding <a href=https://arxiv.org/pdf/2005.04790v2.pdf>paper</a>. \n\n<img src=https://drivendata-public-assets.s3.amazonaws.com/memes-overview.png />\nImage from <a href=https://www.drivendata.org/competitions/64/hateful-memes/>DrivenData</a>","metadata":{}},{"cell_type":"markdown","source":"## Data Overview","metadata":{}},{"cell_type":"markdown","source":"During the competition, a downloadable zip file was provided. Now competition data can be found at this <a href=https://hatefulmemeschallenge.com/>link</a>.\n\nThe zip file includes a folder with images and several json files with images annotations.\n\n<b>img/</b> folder contains all the images of the challenge dataset including train, dev and test split. The images are named \\<id>.png, where \\<id> is a unique 5 digit number.  \n\n**train.jsonl, dev_seen.jsonl, dev_unseen.jsonl** — json files where each line has a dictionary of key-value pairs of data about the images. The dictionary includes\n\n- **id** The unique identifier between the img directory and the .jsonl files, e.g., \"id\": 13894.\n- **img** The actual meme filename, e.g., \"img\": img/13894.png, note that the filename contains the img directory described above, and that the filename stem is the id.\n- **text** The raw text string embedded in the meme image, e.g., img/13894.png has \"text\": \"putting bows on your pet\"\n- **label** where 1 -> \"hateful\" and 0 -> \"non-hateful\"\n\n\nFor example:\n\n{\"id\":23058,\"img\":\"img\\/23058.png\",\"label\":0,\"text\":\"don't be afraid to love again everyone is not like your ex\"}\n\n**test_seen.jsnol** includes mentioned keys, except <b>label</b>.","metadata":{}},{"cell_type":"markdown","source":"## Performance metric","metadata":{}},{"cell_type":"markdown","source":"Model performance and leaderboard rankings were determined using the **AUC ROC** or the Area Under the Curve of the Receiver Operating Characteristic. The metric measures how well your binary classifier discriminates between the classes as its decision threshold is varied. \n\n<img src=images/auroc.png />\n\nAnother metric was the accuracy of  predictions, given by the ratio of correct predictions to the total number of predictions made. \n\n<img src=images/accuracy.png />","metadata":{}},{"cell_type":"markdown","source":"## Our approach","metadata":{}},{"cell_type":"markdown","source":"There are many models and frameworks for working with multimodal data, the one that stands out the most is <a href=https://mmf.sh/>MMF</a> from Facebook. <a href=https://mmf.sh/>MMF</a> provides a simple interface for accessing many powerful multimodal models. But we, as big fans of <a href=https://github.com/huggingface/transformers>Huggingface Transformers</a>, decided not to go the easy way. We decided to find out what multimodal models are available in <a href=https://github.com/huggingface/transformers>Transformers</a> and how to get the most out of them. It turned out that only one model of this kind is currently available in Transformers - <b><a href=\"https://huggingface.co/transformers/summary.html#multimodal-models\">Multimodal Bitransformer (MMBT)</a></b>. Making this model work was not so easy, but this made the task only more interesting. \n\n<img width='700px' src='images/MMBT.png'/>\nMMBT architecture, from <a href='https://arxiv.org/abs/1909.02950'>Supervised Multimodal Bitransformers for Classifying Images and Text paper</a>\n\nMMBT fuses information from text and image encoders. <a href='https://huggingface.co/transformers/model_doc/bert.html'>BERT</a> is used as text encoder and <a href='https://pytorch.org/hub/pytorch_vision_resnet/'>ResNet</a> as image encoder. We took advantage of MMBT architecture flexibility and replaced ResNet with <a href=https://github.com/openai/CLIP>CLIP</a> for image encoding.  <b>CLIP</b> pre-trains an image encoder and a text encoder to predict which images were paired with which texts in a dataset. Our assumption was that features from CLIP are more versatile and better suited for a multimodal domain.\n\n<img width='700px' src='https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png'/>\nSummary of CLIP model’s approach, from <a href='https://arxiv.org/abs/2103.00020'>Learning Transferable Visual Models From Natural Language Supervision paper</a>\n\nFor text encoding we used <a href=https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain>bert-base-uncased-hatexplain</a> model which is available in Huggingface Hub. This model was created for hatespeech detection in English, so in our case features from it are better than from <a href=https://huggingface.co/bert-base-uncased>bert-base-uncased</a> that was used in MMBT initially. \n\nFinal MMBT model was finetuned on a train dataset and validated on dev_seen dataset.\n\nWe also augmented texts in train dataset using controlled <a href=\"https://huggingface.co/transformers/model_doc/gpt2.html\">GPT-2</a> and <a href=\"https://github.com/dsfsi/textaugment#eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks\">Easy Data Augmentation</a> method. This increased the accuracy of our model by an additional few percent. Augmentation is beyond the scope of this article, and I will probably write about it separately if you find this article and our approach interesting. ","metadata":{}},{"cell_type":"markdown","source":"## Implementation","metadata":{}},{"cell_type":"markdown","source":"There will be a lot of code in this section of the article, but I will try to explain all the important parts in detail for a better understanding.\n\nFirst, let's import the required libraries. We will need\n\n- Transformers version >=4.8.2\n- Pytorch version 1.8.1\n- torchvision 0.9.1\n- scikit-learn 0.23.2 \n- Pillow >=8.2.0\n- tqdm >= 4.60.0\n- matplotlib >= 3.3.4\n- numpy >=1.19.5\n- <a href=https://github.com/openai/CLIP>CLIP</a> (that can be installed from repository). \n\nCLIP is now <a href=https://huggingface.co/transformers/model_doc/clip.html>accesible</a> directly in Huggingface Transformers, but at the time of the implementation of our approach it was not there yet. To get the most out of our model, we also used the <a href=https://github.com/facebookresearch/madgrad>MADGRAD</a> optimizer. ","metadata":{}},{"cell_type":"code","source":"!pip install madgrad","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:27:38.792100Z","iopub.execute_input":"2024-05-06T10:27:38.792419Z","iopub.status.idle":"2024-05-06T10:28:05.248854Z","shell.execute_reply.started":"2024-05-06T10:27:38.792393Z","shell.execute_reply":"2024-05-06T10:28:05.247906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install clip","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:05.251205Z","iopub.execute_input":"2024-05-06T10:28:05.251963Z","iopub.status.idle":"2024-05-06T10:28:05.256036Z","shell.execute_reply.started":"2024-05-06T10:28:05.251918Z","shell.execute_reply":"2024-05-06T10:28:05.255159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nfrom collections import Counter\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n# import torchvision\n# import torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom madgrad import MADGRAD\n\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\nfrom transformers import CLIPProcessor, CLIPModel\n# import clip\nimport pickle\n\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport copy\nimport time\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    MMBTConfig,\n    MMBTModel,\n    MMBTForClassification,\n    get_linear_schedule_with_warmup,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:05.257107Z","iopub.execute_input":"2024-05-06T10:28:05.257488Z","iopub.status.idle":"2024-05-06T10:28:13.190124Z","shell.execute_reply.started":"2024-05-06T10:28:05.257450Z","shell.execute_reply":"2024-05-06T10:28:13.189380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:13.191209Z","iopub.execute_input":"2024-05-06T10:28:13.191610Z","iopub.status.idle":"2024-05-06T10:28:13.459161Z","shell.execute_reply.started":"2024-05-06T10:28:13.191585Z","shell.execute_reply":"2024-05-06T10:28:13.458211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a variable with available device, which will do all needed computations. We will need a GPU, so our device is CUDA.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:13.461923Z","iopub.execute_input":"2024-05-06T10:28:13.462241Z","iopub.status.idle":"2024-05-06T10:28:13.498602Z","shell.execute_reply.started":"2024-05-06T10:28:13.462214Z","shell.execute_reply":"2024-05-06T10:28:13.497612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load CLIP model and needed preprocessing.","metadata":{}},{"cell_type":"code","source":"! pip install ftfy regex tqdm\n! pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:13.499851Z","iopub.execute_input":"2024-05-06T10:28:13.500695Z","iopub.status.idle":"2024-05-06T10:28:42.174307Z","shell.execute_reply.started":"2024-05-06T10:28:13.500661Z","shell.execute_reply":"2024-05-06T10:28:42.172930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import clip\n\nclip.available_models()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:42.177239Z","iopub.execute_input":"2024-05-06T10:28:42.178128Z","iopub.status.idle":"2024-05-06T10:28:42.669558Z","shell.execute_reply.started":"2024-05-06T10:28:42.178095Z","shell.execute_reply":"2024-05-06T10:28:42.668641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:28:42.670834Z","iopub.execute_input":"2024-05-06T10:28:42.671222Z","iopub.status.idle":"2024-05-06T10:29:37.030176Z","shell.execute_reply.started":"2024-05-06T10:28:42.671189Z","shell.execute_reply":"2024-05-06T10:29:37.029345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Freeze weights of CLIP feature encoder, as we will not finetune it. ","metadata":{}},{"cell_type":"code","source":"# for p in clip_model.parameters():\n#     p.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.031269Z","iopub.execute_input":"2024-05-06T10:29:37.031533Z","iopub.status.idle":"2024-05-06T10:29:37.035381Z","shell.execute_reply.started":"2024-05-06T10:29:37.031511Z","shell.execute_reply":"2024-05-06T10:29:37.034422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize needed variables.","metadata":{}},{"cell_type":"code","source":"num_image_embeds = 5\nnum_labels = 1\ngradient_accumulation_steps = 20\ndata_dir = '/kaggle/input/memedata/hateful_memes'\nmax_seq_length = 80 \nmax_grad_norm = 0.5\ntrain_batch_size = 16\neval_batch_size = 16\nimage_encoder_size = 256\nimage_features_size = 512\nnum_train_epochs = 20","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.036655Z","iopub.execute_input":"2024-05-06T10:29:37.036891Z","iopub.status.idle":"2024-05-06T10:29:37.045109Z","shell.execute_reply.started":"2024-05-06T10:29:37.036870Z","shell.execute_reply":"2024-05-06T10:29:37.044336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a function that will prepare an image for CLIP encoder in a special manner. This function will split image into three tiles (by height or width, depending on the aspect ratio of the image). Finally we will get four vectors after encoding (one vector for each tile and one vector for whole image that was padded to square).","metadata":{}},{"cell_type":"code","source":"def slice_image(im, desired_size):\n    '''\n    Resize and slice image\n    '''\n    old_size = im.size\n    ratio = float(desired_size)/min(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = im.resize(new_size, Image.LANCZOS)    \n    ar = np.array(im)\n    images = []\n    if ar.shape[0] < ar.shape[1]:\n        middle = ar.shape[1] // 2\n        half = desired_size // 2\n        \n        images.append(Image.fromarray(ar[:, :desired_size]))\n        images.append(Image.fromarray(ar[:, middle-half:middle+half]))\n        images.append(Image.fromarray(ar[:, ar.shape[1]-desired_size:ar.shape[1]]))\n    else:\n        middle = ar.shape[0] // 2\n        half = desired_size // 2\n        \n        images.append(Image.fromarray(ar[:desired_size, :]))\n        images.append(Image.fromarray(ar[middle-half:middle+half, :]))\n        images.append(Image.fromarray(ar[ar.shape[0]-desired_size:ar.shape[0], :]))\n\n    return images","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.046132Z","iopub.execute_input":"2024-05-06T10:29:37.046419Z","iopub.status.idle":"2024-05-06T10:29:37.057436Z","shell.execute_reply.started":"2024-05-06T10:29:37.046397Z","shell.execute_reply":"2024-05-06T10:29:37.056637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def resize_pad_image(im, desired_size):\n    '''\n    Resize and pad image to a desired size\n    '''\n    old_size = im.size\n    ratio = float(desired_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = im.resize(new_size, Image.LANCZOS)\n\n    # create a new image and paste the resized on it\n    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n    new_im.paste(im, ((desired_size-new_size[0])//2,\n                        (desired_size-new_size[1])//2))\n\n    return new_im","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.058714Z","iopub.execute_input":"2024-05-06T10:29:37.059013Z","iopub.status.idle":"2024-05-06T10:29:37.071997Z","shell.execute_reply.started":"2024-05-06T10:29:37.058989Z","shell.execute_reply":"2024-05-06T10:29:37.071100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function, that will get image features from CLIP.","metadata":{}},{"cell_type":"code","source":"class ClipEncoderMulti(nn.Module):\n    def __init__(self, num_embeds, num_features=image_features_size):\n        super().__init__()        \n        self.model = clip_model\n        self.num_embeds = num_embeds\n        self.num_features = num_features\n\n    def forward(self, x):\n        # 4x3x288x288 -> 1x4x640\n        out = self.model.encode_image(x.view(-1,3,288,288))\n        out = out.view(-1, self.num_embeds, self.num_features).float()\n#         print(out.shape)\n        return out  # Bx4x640","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.073042Z","iopub.execute_input":"2024-05-06T10:29:37.073310Z","iopub.status.idle":"2024-05-06T10:29:37.082385Z","shell.execute_reply.started":"2024-05-06T10:29:37.073289Z","shell.execute_reply":"2024-05-06T10:29:37.081578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clip_model.vision_model.encoder","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.087232Z","iopub.execute_input":"2024-05-06T10:29:37.087571Z","iopub.status.idle":"2024-05-06T10:29:37.092351Z","shell.execute_reply.started":"2024-05-06T10:29:37.087548Z","shell.execute_reply":"2024-05-06T10:29:37.091554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_encoder = ClipEncoderMulti(4)\n# image = Image.open(\"dataset/img/42953.png\").convert(\"RGB\")\n\n# sliced_images = slice_image(image, 288)\n# sliced_images = [np.array(preprocess(im)) for im in sliced_images] \n\n# image = resize_pad_image(image, image_encoder_size)\n# image = np.array(preprocess(image))\n\n# sliced_images = [image] + sliced_images\n# sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n\n# print(sliced_images.shape)\n# print(image_encoder(sliced_images).shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.093466Z","iopub.execute_input":"2024-05-06T10:29:37.094039Z","iopub.status.idle":"2024-05-06T10:29:37.102206Z","shell.execute_reply.started":"2024-05-06T10:29:37.094015Z","shell.execute_reply":"2024-05-06T10:29:37.101543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# im = Image.open(\"dataset/img/01576.png\").convert(\"RGB\")\n# sliced_images = slice_image(im, 288) \n# for img in sliced_images:\n#     plt.figure()\n#     plt.imshow(img)\n#     print(img.size)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.103287Z","iopub.execute_input":"2024-05-06T10:29:37.103554Z","iopub.status.idle":"2024-05-06T10:29:37.115596Z","shell.execute_reply.started":"2024-05-06T10:29:37.103526Z","shell.execute_reply":"2024-05-06T10:29:37.114829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create JsonlDataset class that will load texts and preprocessed images. <b>collate_fn</b> will group data from dataset in a format needed for our pytorch model.","metadata":{}},{"cell_type":"code","source":"class JsonlDataset(Dataset):\n    def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n        self.data = [json.loads(l) for l in open(data_path)]\n        self.data_dir = os.path.dirname(data_path)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n        sentence = sentence[:self.max_seq_length]\n\n        label = torch.FloatTensor([self.data[index][\"label\"]])\n\n        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n        sliced_images = slice_image(image, 288)\n        sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n        image = resize_pad_image(image, image_encoder_size)\n        image = np.array(self.transforms(image))\n        \n        sliced_images = [image] + sliced_images         \n        sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n\n        return {\n            \"image_start_token\": start_token,            \n            \"image_end_token\": end_token,\n            \"sentence\": sentence,\n            \"image\": sliced_images,\n            \"label\": label            \n        }\n\n    def get_label_frequencies(self):\n        label_freqs = Counter()\n        for row in self.data:\n            label_freqs.update([row[\"label\"]])\n        return label_freqs\n    \n    def get_labels(self):\n        labels = []\n        for row in self.data:\n            labels.append(row[\"label\"])\n        return labels","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.116653Z","iopub.execute_input":"2024-05-06T10:29:37.116956Z","iopub.status.idle":"2024-05-06T10:29:37.129798Z","shell.execute_reply.started":"2024-05-06T10:29:37.116933Z","shell.execute_reply":"2024-05-06T10:29:37.128922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    lens = [len(row[\"sentence\"]) for row in batch]\n    bsz, max_seq_len = len(batch), max(lens)\n\n    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n\n    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n        mask_tensor[i_batch, :length] = 1\n    \n    img_tensor = torch.stack([row[\"image\"] for row in batch])\n    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n\n    return {'text_tensor' :text_tensor, 'mask_tensor' : mask_tensor, 'img_tensor': img_tensor, 'img_start_token' : img_start_token, 'img_end_token' : img_end_token, 'tgt_tensor' : tgt_tensor}","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.131033Z","iopub.execute_input":"2024-05-06T10:29:37.131315Z","iopub.status.idle":"2024-05-06T10:29:37.143134Z","shell.execute_reply.started":"2024-05-06T10:29:37.131292Z","shell.execute_reply":"2024-05-06T10:29:37.142343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define **load_examples** function that will load data described in json dataset into JsonlDataset object.","metadata":{}},{"cell_type":"code","source":"def load_examples(tokenizer, evaluate=False):\n    path = os.path.join(data_dir, \"dev_seen.jsonl\" if evaluate else f\"train.jsonl\")\n    transforms = preprocess\n    dataset = JsonlDataset(path, tokenizer, transforms, max_seq_length - num_image_embeds - 2)\n    print(dataset)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.144317Z","iopub.execute_input":"2024-05-06T10:29:37.145267Z","iopub.status.idle":"2024-05-06T10:29:37.156176Z","shell.execute_reply.started":"2024-05-06T10:29:37.145235Z","shell.execute_reply":"2024-05-06T10:29:37.155326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create functions to load and save model weights.","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(save_path, model, valid_loss):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'model_state_dict': model.state_dict(),\n                  'valid_loss': valid_loss}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n    \ndef load_checkpoint(load_path, model):\n    \n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    model.load_state_dict(state_dict['model_state_dict'])\n    return state_dict['valid_loss']","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.157429Z","iopub.execute_input":"2024-05-06T10:29:37.157948Z","iopub.status.idle":"2024-05-06T10:29:37.167136Z","shell.execute_reply.started":"2024-05-06T10:29:37.157918Z","shell.execute_reply":"2024-05-06T10:29:37.166313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Needed functions and classes are created, so we can load our models.","metadata":{}},{"cell_type":"code","source":"model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\ntransformer_config = AutoConfig.from_pretrained(model_name) \ntransformer = AutoModel.from_pretrained(model_name, config=transformer_config)\nimg_encoder = ClipEncoderMulti(num_image_embeds)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:37.168203Z","iopub.execute_input":"2024-05-06T10:29:37.168520Z","iopub.status.idle":"2024-05-06T10:29:40.437342Z","shell.execute_reply.started":"2024-05-06T10:29:37.168489Z","shell.execute_reply":"2024-05-06T10:29:40.436506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:40.438638Z","iopub.execute_input":"2024-05-06T10:29:40.438928Z","iopub.status.idle":"2024-05-06T10:29:41.414473Z","shell.execute_reply.started":"2024-05-06T10:29:40.438904Z","shell.execute_reply":"2024-05-06T10:29:41.413661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = MMBTConfig(transformer_config, num_labels=num_labels, modal_hidden_size=image_features_size)\nmodel = MMBTForClassification(config, transformer, img_encoder)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.415627Z","iopub.execute_input":"2024-05-06T10:29:41.415941Z","iopub.status.idle":"2024-05-06T10:29:41.424691Z","shell.execute_reply.started":"2024-05-06T10:29:41.415914Z","shell.execute_reply":"2024-05-06T10:29:41.423854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for p in model.mmbt.modal_encoder.encoder.model.parameters():\n    if p.requires_grad == True:\n        print(p)\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.425827Z","iopub.execute_input":"2024-05-06T10:29:41.426171Z","iopub.status.idle":"2024-05-06T10:29:41.682904Z","shell.execute_reply.started":"2024-05-06T10:29:41.426133Z","shell.execute_reply":"2024-05-06T10:29:41.682012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for p in model.parameters():\n#     print(p)\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.684014Z","iopub.execute_input":"2024-05-06T10:29:41.684332Z","iopub.status.idle":"2024-05-06T10:29:41.688064Z","shell.execute_reply.started":"2024-05-06T10:29:41.684307Z","shell.execute_reply":"2024-05-06T10:29:41.687289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device);","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.689411Z","iopub.execute_input":"2024-05-06T10:29:41.689700Z","iopub.status.idle":"2024-05-06T10:29:41.822954Z","shell.execute_reply.started":"2024-05-06T10:29:41.689677Z","shell.execute_reply":"2024-05-06T10:29:41.822145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print('multiple devices')\n    model = nn.DataParallel(model).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.824080Z","iopub.execute_input":"2024-05-06T10:29:41.824430Z","iopub.status.idle":"2024-05-06T10:29:41.829235Z","shell.execute_reply.started":"2024-05-06T10:29:41.824404Z","shell.execute_reply":"2024-05-06T10:29:41.828218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train and evaluation datasets and create dataloaders for these datasets.","metadata":{}},{"cell_type":"code","source":"train_dataset = load_examples(tokenizer, evaluate=False)\neval_dataset = load_examples(tokenizer, evaluate=True)   \n\ntrain_sampler = RandomSampler(train_dataset)\neval_sampler = SequentialSampler(eval_dataset)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=train_batch_size,\n        collate_fn=collate_fn\n    )\n\n\neval_dataloader = DataLoader(\n        eval_dataset, \n        sampler=eval_sampler, \n        batch_size=eval_batch_size, \n        collate_fn=collate_fn\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.830417Z","iopub.execute_input":"2024-05-06T10:29:41.832375Z","iopub.status.idle":"2024-05-06T10:29:41.917869Z","shell.execute_reply.started":"2024-05-06T10:29:41.832343Z","shell.execute_reply":"2024-05-06T10:29:41.916902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x in train_dataloader:\n#     print(x['img_tensor'])\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.919033Z","iopub.execute_input":"2024-05-06T10:29:41.919334Z","iopub.status.idle":"2024-05-06T10:29:41.923305Z","shell.execute_reply.started":"2024-05-06T10:29:41.919309Z","shell.execute_reply":"2024-05-06T10:29:41.922402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define model training parameters, optimizer and loss.","metadata":{}},{"cell_type":"code","source":"# Prepare optimizer and schedule (linear warmup and decay)\nno_decay = [\"bias\", \n            \"LayerNorm.weight\"\n           ]\nweight_decay = 0.0005\n\noptimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n\nt_total = (len(train_dataloader) // gradient_accumulation_steps) * num_train_epochs\nwarmup_steps = t_total // 10\n\noptimizer = MADGRAD(optimizer_grouped_parameters, lr=2e-4)\n\nscheduler = get_linear_schedule_with_warmup(\n        optimizer, warmup_steps, t_total\n    )\n\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.924380Z","iopub.execute_input":"2024-05-06T10:29:41.924625Z","iopub.status.idle":"2024-05-06T10:29:41.943330Z","shell.execute_reply.started":"2024-05-06T10:29:41.924604Z","shell.execute_reply":"2024-05-06T10:29:41.942538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define evaluation function that will take evaluation dataloader and calculate prediction AUC, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 score</a> and accuracy.","metadata":{}},{"cell_type":"code","source":"def evaluate(model, tokenizer, criterion, dataloader, tres = 0.5): \n    \n    # Eval!\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    proba = None\n    out_label_ids = None\n    for batch in dataloader:\n        model.eval()\n        for key,values in batch.items():\n            batch[key] = values.to(device)\n        with torch.no_grad():\n            labels = batch['tgt_tensor']\n            inputs = {\n            \"input_ids\": batch['text_tensor'],\n            \"input_modal\": batch['img_tensor'],\n            \"attention_mask\": batch['mask_tensor'],\n            \"modal_start_tokens\": batch['img_start_token'],\n            \"modal_end_tokens\": batch['img_end_token'],\n            \"return_dict\": False\n            }\n            outputs = model(**inputs)\n            logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n            tmp_eval_loss = criterion(logits, labels)\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = torch.sigmoid(logits).detach().cpu().numpy() > tres\n            proba = torch.sigmoid(logits).detach().cpu().numpy()\n            out_label_ids = labels.detach().cpu().numpy()\n        else:            \n            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > tres, axis=0)\n            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n    \n    eval_loss = eval_loss / nb_eval_steps\n\n    result = {\n        \"loss\": eval_loss,\n        \"accuracy\": accuracy_score(out_label_ids, preds),\n        \"AUC\": roc_auc_score(out_label_ids, proba),\n        \"micro_f1\": f1_score(out_label_ids, preds, average=\"micro\"),\n        \"prediction\": preds,\n        \"labels\": out_label_ids,\n        \"proba\": proba\n    }\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:29:41.944418Z","iopub.execute_input":"2024-05-06T10:29:41.944677Z","iopub.status.idle":"2024-05-06T10:29:41.958553Z","shell.execute_reply.started":"2024-05-06T10:29:41.944655Z","shell.execute_reply":"2024-05-06T10:29:41.957705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer_step = 0\nglobal_step = 0\ntrain_step = 0\ntr_loss, logging_loss = 0.0, 0.0\nbest_valid_auc = 0.5\nglobal_steps_list = []\ntrain_loss_list = []\nval_loss_list = []\nval_acc_list = []\nval_auc_list = []\neval_every = len(train_dataloader) // 7\nrunning_loss = 0\nfile_path=\"/kaggle/working/\"\n\nmodel.zero_grad()\n\nfor i in range(num_train_epochs):\n    start = time.time()\n    print(\"Epoch\", i+1, f\"from {num_train_epochs}\")\n    whole_y_pred=np.array([])\n    whole_y_t=np.array([])\n    for step, batch in enumerate(train_dataloader):\n        model.train()\n        running_loss = 0.0 \n        for key,values in batch.items():\n            batch[key] = values.to(device)\n        labels = batch['tgt_tensor']\n#         print(batch['img_tensor'].shape)\n        inputs = {\n            \"input_ids\": batch['text_tensor'].to(device),\n            \"input_modal\": batch['img_tensor'].to(device),\n            \"attention_mask\": batch['mask_tensor'].to(device),\n            \"modal_start_tokens\": batch['img_start_token'].to(device),\n            \"modal_end_tokens\": batch['img_end_token'].to(device),\n            \"return_dict\": False\n        }\n        outputs = model(**inputs)\n        logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n        loss = criterion(logits, labels)        \n        \n        if gradient_accumulation_steps > 1:\n            loss = loss / gradient_accumulation_steps\n            \n        loss.backward()\n        \n        tr_loss += loss.item()\n        running_loss += loss.item()\n        global_step += 1\n        \n        if (step + 1) % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()  # Update learning rate schedule         \n            \n            optimizer_step += 1\n            optimizer.zero_grad()   \n                        \n        if (step + 1) % eval_every == 0:\n            \n            average_train_loss = running_loss / eval_every\n            train_loss_list.append(average_train_loss)\n            global_steps_list.append(global_step)\n            running_loss = 0.0  \n            \n            val_result = evaluate(model, tokenizer, criterion, eval_dataloader)\n            \n            val_loss_list.append(val_result['loss'])\n            val_acc_list.append(val_result['accuracy'])\n            val_auc_list.append(val_result['AUC'])\n            \n            # checkpoint\n            if val_result['AUC'] > best_valid_auc:\n                best_valid_auc = val_result['AUC']\n                val_loss = val_result['loss']\n                val_acc = val_result['accuracy']\n                model_path = f'{file_path}/model.pth'\n                print(f\"AUC improved, so saving this model\")  \n                save_checkpoint(model_path, model, val_result['loss'])              \n            \n            print(\"Train loss:\", f\"{average_train_loss:.4f}\", \n                  \"Val loss:\", f\"{val_result['loss']:.4f}\",\n                  \"Val acc:\", f\"{val_result['accuracy']:.4f}\",\n                  \"AUC:\", f\"{val_result['AUC']:.4f}\",\n                 'time:', f'{time.time() - start}')   \n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:55:55.801555Z","iopub.execute_input":"2024-05-06T10:55:55.802329Z","iopub.status.idle":"2024-05-06T11:27:29.016310Z","shell.execute_reply.started":"2024-05-06T10:55:55.802292Z","shell.execute_reply":"2024-05-06T11:27:29.014908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nitem = {'train loss' : train_loss_list, 'val loss' : val_loss_list, 'val accuracy' : val_acc_list, 'val_auc' : val_auc_list}\ndf = pd.DataFrame(item)\ndf.to_csv('/kaggle/working/stats.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.805402Z","iopub.status.idle":"2024-05-06T10:51:13.805749Z","shell.execute_reply.started":"2024-05-06T10:51:13.805584Z","shell.execute_reply":"2024-05-06T10:51:13.805599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we can train our model. We specify minimun needed AUC value in <b>best_valid_auc</b> variable, so if the model achieves higher AUC on validation data than it was specified, we will save that model.","metadata":{}},{"cell_type":"code","source":"# optimizer_step = 0\n# global_step = 0\n# train_step = 0\n# tr_loss, logging_loss = 0.0, 0.0\n# best_valid_auc = 0\n# global_steps_list = []\n# train_loss_list = []\n# val_loss_list = []\n# val_acc_list = []\n# val_auc_list = []\n# eval_every = len(train_dataloader)\n# running_loss = 0\n# file_path=\"/kaggle/working/\"\n\n# model.zero_grad()\n\n# for i in range(num_train_epochs):\n#     print(\"Epoch\", i+1, f\"from {num_train_epochs}\")\n#     whole_y_pred=np.array([])\n#     whole_y_t=np.array([])\n#     preds = []\n#     out_label_ids = []\n#     for step, batch in enumerate(tqdm(train_dataloader)):\n#         model.train()\n# #         batch = tuple(values.to(device) for key,values in batch.items())\n#         for key,values in batch.items():\n#             batch[key] = values.to(device)\n#         labels = batch['tgt_tensor']\n# #         print(batch['img_tensor'].shape)\n#         inputs = {\n#             \"input_ids\": batch['text_tensor'].to(device),\n#             \"input_modal\": batch['img_tensor'].to(device),\n#             \"attention_mask\": batch['mask_tensor'].to(device),\n#             \"modal_start_tokens\": batch['img_start_token'].to(device),\n#             \"modal_end_tokens\": batch['img_end_token'].to(device),\n#             \"return_dict\": False\n#         }\n# #         print(inputs)\n#         outputs = model(**inputs)\n#         logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n# #         print(logits.shape)\n# #         print(logits)\n#         loss = criterion(logits, labels)        \n        \n# #         if gradient_accumulation_steps > 1:\n# #             loss = loss / gradient_accumulation_steps\n         \n#         optimizer.zero_grad()\n#         loss.backward()\n        \n#         tr_loss += loss.item()\n#         running_loss += loss.item()\n#         global_step += 1\n#         preds.append((torch.sigmoid(logits).detach().cpu().numpy() > 0.5).astype(int))\n            \n#         out_label_ids.append(labels.detach().cpu().numpy())\n# #         print(preds,out_label_ids)\n# #         if (step + 1) % gradient_accumulation_steps == 0:\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n#         optimizer.step()\n#     scheduler.step()  # Update learning rate schedule         \n            \n            \n            \n           \n                        \n# #         if (step + 1) % eval_every == 0:\n            \n#     average_train_loss = running_loss / eval_every\n#     train_loss_list.append(average_train_loss)\n#     train_accuracy = accuracy_score(out_label_ids, preds)\n# #         global_steps_list.append(global_step)\n#     running_loss = 0.0  \n            \n#     val_result = evaluate(model, tokenizer, criterion, eval_dataloader)\n            \n#     val_loss_list.append(val_result['loss'])\n#     val_acc_list.append(val_result['accuracy'])\n#     val_auc_list.append(val_result['AUC'])\n            \n#             # checkpoint\n#     if val_result['AUC'] > best_valid_auc:\n#                 best_valid_auc = val_result['AUC']\n#                 val_loss = val_result['loss']\n#                 val_acc = val_result['accuracy']\n#                 model_path = f'{file_path}/model-embs{num_image_embeds}-seq{max_seq_length}-auc{best_valid_auc:.3f}-loss{val_loss:.3f}-acc{val_acc:.3f}.pth'\n#                 print(f\"AUC improved, so saving this model\")  \n#                 save_checkpoint(model_path, model, val_result['loss'])              \n            \n#     print(\"Train loss:\", f\"{average_train_loss:.4f}\", \n#           'train accuracy:', f'{train_accuracy}',\n#                   \"Val loss:\", f\"{val_result['loss']:.4f}\",\n#                   \"Val acc:\", f\"{val_result['accuracy']:.4f}\",\n#                   \"AUC:\", f\"{val_result['AUC']:.4f}\")   \n#     print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.806898Z","iopub.status.idle":"2024-05-06T10:51:13.807228Z","shell.execute_reply.started":"2024-05-06T10:51:13.807049Z","shell.execute_reply":"2024-05-06T10:51:13.807062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# item = {'train_loss' : train_loss_list, 'train_accuracy' : train_accuracy,'val_loss' : val_result['loss'], 'val_accuracy' : val_result['accuracy'], ' AUC' : val_result['AUC']}","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.808668Z","iopub.status.idle":"2024-05-06T10:51:13.809091Z","shell.execute_reply.started":"2024-05-06T10:51:13.808867Z","shell.execute_reply":"2024-05-06T10:51:13.808885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training is complete, we can visualize results:","metadata":{}},{"cell_type":"code","source":"# plt.plot(global_steps_list, val_auc_list)\n# plt.grid()\n# plt.xlabel('Global Steps')\n# plt.ylabel('AUC')\n# plt.title('MMBT Area Under the Curve')\n# plt.show() ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.810541Z","iopub.status.idle":"2024-05-06T10:51:13.811018Z","shell.execute_reply.started":"2024-05-06T10:51:13.810773Z","shell.execute_reply":"2024-05-06T10:51:13.810794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions for submission","metadata":{}},{"cell_type":"markdown","source":"Now we can make a prediction for test data. Firstly we will create needed classes and functions for data loading and processing by analogy with the training stage.","metadata":{}},{"cell_type":"code","source":"# import gc\n# import torch\n\n# # Clear Python's internal garbage collector\n# gc.collect()\n\n# # If we're using PyTorch with CUDA, clear the GPU memory as well\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n#     torch.cuda.synchronize()  # wait for cuda to finish (cuda operations are asynchronous)\n#     torch.cuda.ipc_collect() ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.812041Z","iopub.status.idle":"2024-05-06T10:51:13.812493Z","shell.execute_reply.started":"2024-05-06T10:51:13.812258Z","shell.execute_reply":"2024-05-06T10:51:13.812278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_labels = 1\n# data_dir = './dataset'\n# test_batch_size = 16","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:51:13.814230Z","iopub.status.idle":"2024-05-06T10:51:13.814663Z","shell.execute_reply.started":"2024-05-06T10:51:13.814441Z","shell.execute_reply":"2024-05-06T10:51:13.814460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"def slice_image(im, desired_size):\n    '''\n    Resize and slice image\n    '''\n    old_size = im.size\n    ratio = float(desired_size)/min(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = im.resize(new_size, Image.ANTIALIAS)    \n    ar = np.array(im)\n    images = []\n    if ar.shape[0] < ar.shape[1]:\n        middle = ar.shape[1] // 2\n        half = desired_size // 2\n        \n        images.append(Image.fromarray(ar[:, :desired_size]))\n        images.append(Image.fromarray(ar[:, middle-half:middle+half]))\n        images.append(Image.fromarray(ar[:, ar.shape[1]-desired_size:ar.shape[1]]))\n    else:\n        middle = ar.shape[0] // 2\n        half = desired_size // 2\n        \n        images.append(Image.fromarray(ar[:desired_size, :]))\n        images.append(Image.fromarray(ar[middle-half:middle+half, :]))\n        images.append(Image.fromarray(ar[ar.shape[0]-desired_size:ar.shape[0], :]))\n\n    return imagesIn this article I tried to describe in detail the concept and implementation of the approach that we used in Hateful Memes Competition from Facebook. The tasks that the competition set for us turned out to be extremely interesting and we had a lot of fun developing our approach to solve these tasks. I hope you enjoyed reading this article too. \n\nI also want to mention that to obtain maximum AUC, we combined the prediction of several variants of this model trained with different loss parameters and different augmentation options. But that's a topic for an entirely different article. ","metadata":{}}]}