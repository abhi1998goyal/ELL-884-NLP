{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8118392,"sourceType":"datasetVersion","datasetId":4796795}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. Longformer can handle long texts. In other models text was getting trimmed , there are text more than 5000 tokens in dataset. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom transformers import LongformerTokenizer, LongformerModel, LongformerConfig\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Read the dataset\ndf = pd.read_csv(\"/kaggle/input/comp-4/comp_1.csv\")\n\n# Concatenate title and abstract text\ntexts = df['Title'] + \" \" + df['abstractText']\ntexts = texts.astype(str)\nlabels = df[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']]\n\n# Split dataset into train, validation, and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1)\n\n# Load Longformer tokenizer and encode text data\ntokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\ntrain_encodings = tokenizer(list(train_texts), padding=True, truncation=True, max_length=512, return_tensors='pt')\nval_encodings = tokenizer(list(val_texts), padding=True, truncation=True, max_length=512, return_tensors='pt')\ntest_encodings = tokenizer(list(test_texts), padding=True, truncation=True, max_length=512, return_tensors='pt')\n\n# Convert labels to tensors\ntrain_labels_tensor = torch.tensor(np.array(train_labels), dtype=torch.float32)\nval_labels_tensor = torch.tensor(np.array(val_labels), dtype=torch.float32)\ntest_labels_tensor = torch.tensor(np.array(test_labels), dtype=torch.float32)\n\n# Create PyTorch DataLoader\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\nval_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels_tensor)\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define Longformer-based model\nclass LongformerClassifier(nn.Module):\n    def __init__(self):\n        super(LongformerClassifier, self).__init__()\n        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n        self.dropout = nn.Dropout(0.1)\n        self.fc_layers = nn.ModuleList([nn.Linear(self.longformer.config.hidden_size, self.longformer.config.hidden_size) for _ in range(1)])\n        self.output_layer = nn.Linear(self.longformer.config.hidden_size, 14)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0, :]  # Using the first token's representation (CLS token)\n        pooled_output = self.dropout(pooled_output)\n        for i in range(1):\n            pooled_output = self.fc_layers[i](pooled_output)\n            pooled_output = nn.ReLU()(pooled_output)\n        logits = self.output_layer(pooled_output)\n        return logits\n\n# Instantiate model, optimizer, and loss function\nmodel = LongformerClassifier().to(device)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\ncriterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for multilabel classification\n\n# Training loop\ndef train_epoch(model, train_loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\n# Validation loop\ndef validate(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0\n    all_preds = []\n    all_targets = []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, attention_mask, labels = batch\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels)\n            val_loss += loss.item()\n            all_preds.extend(torch.sigmoid(logits).cpu().numpy())\n            all_targets.extend(labels.cpu().numpy())\n    val_loss /= len(val_loader)\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    return val_loss, all_preds, all_targets\n\n# Training loop with early stopping\nbest_val_loss = float('inf')\npatience = 3  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor epoch in range(15):  # Adjust number of epochs as needed\n    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n    val_loss, val_preds, val_targets = validate(model, val_loader, criterion)\n    val_f1_micro = f1_score(val_targets, np.round(val_preds), average='micro')\n    print(f\"Epoch {epoch + 1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val F1 Micro {val_f1_micro:.4f}\")\n    \n    # Check for early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement = 0\n        torch.save(model.state_dict(), '/kaggle/working/best_longformer_model.pth')  # Save the best model weights\n    else:\n        no_improvement += 1\n        if no_improvement >= patience:\n            print(\"Early stopping triggered. No improvement in validation loss.\")\n            break\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Test loop\nmodel.load_state_dict(torch.load('/kaggle/working/best_longformer_model.pth'))  # Load the best model weights\nmodel.eval()\nall_preds = []\nall_targets = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n        logits = model(input_ids, attention_mask)\n        all_preds.extend(torch.sigmoid(logits).cpu().numpy())\n        all_targets.extend(labels.cpu().numpy())\nall_preds = np.array(all_preds)\nall_targets = np.array(all_targets)\n\n# Compute evaluation metrics\ntest_precision = []\ntest_recall = []\ntest_f1 = []\nfor i in range(14):\n    class_preds = np.round(all_preds[:, i])\n    class_targets = all_targets[:, i]\n    precision = precision_score(class_targets, class_preds)\n    recall = recall_score(class_targets, class_preds)\n    f1 = f1_score(class_targets, class_preds)\n    test_precision.append(precision)\n    test_recall.append(recall)\n    test_f1.append(f1)\n\nprint(\"Test Metrics:\")\nlabel_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\nfor i, label in enumerate(label_names):\n    print(f\"{label}: Precision: {test_precision[i]:.4f}, Recall: {test_recall[i]:.4f}, F1 Score: {test_f1[i]:.4f}\")\n\n# Calculate evaluation metrics\ntest_f1_micro = f1_score(all_targets, np.round(all_preds), average='micro')\ntest_precision_micro = precision_score(all_targets, np.round(all_preds), average='micro')\ntest_recall_micro = recall_score(all_targets, np.round(all_preds), average='micro')\ntest_accuracy = accuracy_score(all_targets, np.round(all_preds))\nprint(f\"Test F1 Micro: {test_f1_micro:.4f}, Test Precision Micro: {test_precision_micro:.4f}, \"\n      f\"Test Recall Micro: {test_recall_micro:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n\n\ntest_f1_macro = f1_score(all_targets, np.round(all_preds), average='macro')\ntest_precision_macro = precision_score(all_targets, np.round(all_preds), average='macro')\ntest_recall_macro = recall_score(all_targets, np.round(all_preds), average='macro')\ntest_accuracy = accuracy_score(all_targets, np.round(all_preds))\nprint(f\"Test F1 Macro: {test_f1_macro:.4f}, Test Precision Macro: {test_precision_macro:.4f}, \"\n      f\"Test Recall Macro: {test_recall_macro:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}